env {
  execution.parallelism = 1
  spark.sql.catalogImplementation = "hive"
}

source {
  hive {
    pre_sql = "select * from mydb.mytb"
  }
}

transform {
  sql {
    sql = "select name,age from fake"
  }
}

sink {
  ClickhouseFile {
    host = "10.60.94.1:8123,10.60.94.2:8123,10.60.94.3:8123"
    database = "mydb"
    table = "access_msg"
    fields = ["date", "datetime", "hostname", "http_code", "data_size", "ua", "request_time"]
    username = "username"
    password = "password"
    sharding_key = "age"
    clickhouse_local_path = "/usr/bin/clickhouse-local"
    node_pass = [
      {
        node_address = "10.60.94.1"
        username = "clickhouse"
        password = "password"
      }
      {
        node_address = "10.60.94.2"
        password = "password"
      }
      {
        node_address = "10.60.94.3"
        password = "password"
      }
    ]
}
}